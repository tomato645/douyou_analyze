{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL\n",
    "URL = \"https://www.uta-net.com/artist/4002/4/\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "r = requests.get(URL)\n",
    "\n",
    "# Write the response content to a file named \"home.html\"\n",
    "with open(\"home.html\", \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"home.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Find all the links with class \"py-2 py-lg-0\" and create absolute URLs\n",
    "    links = [f\"https://www.uta-net.com{link.get('href')}\" for link in soup.find_all(\"a\", class_=\"py-2 py-lg-0\")]\n",
    "\n",
    "# Write the links to a CSV file\n",
    "with open(\"links.csv\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for link in links:\n",
    "        file.write(f\"{link}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Read URLs from links.csv\n",
    "url_list = []\n",
    "with open(\"links.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "    # get the number of items in csv\n",
    "    for url in csv_file:\n",
    "        clean_url = url.strip()\n",
    "        url_list.append(clean_url)\n",
    "\n",
    "# if the html_files folder exsist, Count the number of files in the html_files folder\n",
    "if os.path.exists(\"html_files\"):\n",
    "    html_files_count = sum(len(files) for _, _, files in os.walk(\"html_files\"))\n",
    "else:\n",
    "    # make directory\n",
    "    os.mkdir(\"html_files\")\n",
    "    html_files_count = 0\n",
    "\n",
    "# Download html files from clean_url\n",
    "for clean_url in url_list[html_files_count:]:\n",
    "    print(f\"Downloading {clean_url}\")\n",
    "    response = requests.get(clean_url)\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    with open(f\"./html_files/{current_time}.html\", \"wb\") as html_file:\n",
    "        html_file.write(response.content)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_files = glob.glob(\"html_files/*.html\")\n",
    "\n",
    "kashi_list = []\n",
    "for html_file in html_files:\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        kashi_area = soup.find(\"div\", {\"id\": \"kashi_area\"})\n",
    "        if kashi_area:\n",
    "            kashi_text = kashi_area.text\n",
    "            kashi_list.append(kashi_text)\n",
    "\n",
    "# Write kashi_text to kashi.csv separated by commas\n",
    "with open(\"kashi.csv\", \"w\", encoding=\"utf-8\") as kashi_file:\n",
    "    for kashi_text in kashi_list:\n",
    "        kashi_file.write(f\"{kashi_text},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Japanese part of speech\n",
    "with open(\"kashi.csv\", \"r\", encoding=\"utf-8\") as kashi_file:\n",
    "    combined_string = \",\".join(line.strip() for line in kashi_file)\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "import csv\n",
    "\n",
    "text = combined_string\n",
    "tokenizer = Tokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Prepare data for CSV\n",
    "rows = [[token.surface, token.part_of_speech] for token in tokens]\n",
    "\n",
    "# Write to CSV\n",
    "with open('kashi_janome.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Token\", \"Part of Speech\"])  # Writing headers\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract any part of speech from kashi_janome.csv\n",
    "import csv\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def extract_pos(pos=\"名詞\", output_file=\"kashi_nouns.csv\"):\n",
    "    pos_list = []\n",
    "    with open(\"kashi_janome.csv\", \"r\", newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip headers\n",
    "        for row in reader:\n",
    "            if pos in row[1]:\n",
    "                pos_list.append(row[0])\n",
    "\n",
    "    print(pos_list)\n",
    "\n",
    "    # save nouns to csv\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([pos_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_pos(\"名詞\", \"kashi_nouns.csv\")\n",
    "extract_pos(\"動詞\", \"kashi_verbs.csv\")\n",
    "extract_pos(\"形容詞\", \"kashi_adjective.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def word_cloud(csv_path='kashi_nouns.csv', output_dir=\"nouns\", probability_threshold=0.4, exclude_words=[]):\n",
    "    nouns = []\n",
    "\n",
    "    # Read nouns from kashi_nouns.csv\n",
    "    with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Caution: random sampling is used here!!!!!!!!!\n",
    "            if random.random() < probability_threshold:\n",
    "                nouns.extend(row)\n",
    "\n",
    "    # If nouns is empty, exit\n",
    "    if not nouns:\n",
    "        print(\"No nouns found because of randomness.\")\n",
    "        return\n",
    "\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=900,\n",
    "        height=900,\n",
    "        background_color='white',\n",
    "        font_path='Noto_Sans_JP/NotoSansJP-VariableFont_wght.ttf',\n",
    "        stopwords=exclude_words\n",
    "    ).generate(' '.join(nouns))\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    folder_name = \"wordCloud\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    if not os.path.exists(f\"{folder_name}/{output_dir}\"):\n",
    "        os.mkdir(f\"{folder_name}/{output_dir}\")\n",
    "\n",
    "    # Save word cloud image\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    wordcloud.to_file(f\"./wordcloud/{output_dir}/{current_time}.png\")\n",
    "\n",
    "    # Display word cloud\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 10\n",
    "for _ in range(NUM):\n",
    "    # You can change arguments\n",
    "    word_cloud(\"kashi_nouns.csv\", \"nouns\", 0.6, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bar_word_count(csv_file_path='kashi_nouns.csv', title=\"Noun Count\", max_words=10, exclude_word=[]):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data_frame = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "    all_words = data_frame.values.flatten()\n",
    "    all_words = [str(word).strip() for word in all_words if pd.notnull(word)]\n",
    "    all_words = [word for word in all_words if word not in exclude_word]\n",
    "\n",
    "    word_counts = pd.Series(all_words).value_counts()[:max_words]\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    folder_name = \"barGraph\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "\n",
    "    # Set the font for the plot\n",
    "    plt.rcParams['font.family'] = \"MS Gothic\"\n",
    "    word_counts.plot(kind='bar', color='skyblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(title)\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    plt.savefig(f\"{folder_name}/{title}_{current_time}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_word_count(\"kashi_nouns.csv\", \"名詞使用頻度\", 10, [\"(\", \")\", \",\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_word_count(\"kashi_adjective.csv\", \"形容詞使用頻度\", 17, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_word_count(\"kashi_verbs.csv\", \"動詞使用頻度\", 17, [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
