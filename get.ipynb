{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL\n",
    "URL = \"https://www.uta-net.com/artist/4002/4/\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "r = requests.get(URL)\n",
    "\n",
    "# Write the response content to a file named \"home.html\"\n",
    "with open(\"home.html\", \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read home.html\n",
    "with open(\"home.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Find all the links with class \"py-2 py-lg-0\" and create absolute URLs\n",
    "    links = [f\"https://www.uta-net.com{link.get('href')}\" for link in soup.find_all(\"a\", class_=\"py-2 py-lg-0\")]\n",
    "\n",
    "# Write the links to a CSV file\n",
    "with open(\"links.csv\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for link in links:\n",
    "        file.write(f\"{link}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Read URLs from links.csv\n",
    "url_list = []\n",
    "with open(\"links.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "    for url in csv_file:\n",
    "        clean_url = url.strip()\n",
    "        url_list.append(clean_url)\n",
    "\n",
    "# if the html_files folder exsist, Count the number of files in the html_files folder\n",
    "if os.path.exists(\"html_files\"):\n",
    "    html_files_count = sum(len(files) for _, _, files in os.walk(\"html_files\"))\n",
    "else:\n",
    "    # make directory\n",
    "    os.mkdir(\"html_files\")\n",
    "    html_files_count = 0\n",
    "\n",
    "# Download html files from clean_url\n",
    "for clean_url in url_list[html_files_count:]:\n",
    "    print(f\"Downloading {clean_url}\")\n",
    "    response = requests.get(clean_url)\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    with open(f\"./html_files/{current_time}.html\", \"wb\") as html_file:\n",
    "        html_file.write(response.content)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_files = glob.glob(\"html_files/*.html\")\n",
    "\n",
    "kashi_list = []\n",
    "for html_file in html_files:\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        kashi_area = soup.find(\"div\", {\"id\": \"kashi_area\"})\n",
    "        if kashi_area:\n",
    "            kashi_text = kashi_area.text\n",
    "            kashi_list.append(kashi_text)\n",
    "\n",
    "# Write kashi_text to kashi.csv separated by commas\n",
    "with open(\"kashi.csv\", \"w\", encoding=\"utf-8\") as kashi_file:\n",
    "    for kashi_text in kashi_list:\n",
    "        kashi_file.write(f\"{kashi_text},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open kashi.csv and combine one string variable without pandas\n",
    "with open(\"kashi.csv\", \"r\", encoding=\"utf-8\") as kashi_file:\n",
    "    combined_string = \",\".join(line.strip() for line in kashi_file)\n",
    "print(combined_string)\n",
    "\n",
    "# morphological analysis of japanese with python exclude mecab\n",
    "# You can perform morphological analysis of Japanese text in Python using libraries like Janome or SudachiPy, which can exclude MeCab as an option.\n",
    "# Here is an example using Janome:\n",
    "from janome.tokenizer import Tokenizer\n",
    "import csv\n",
    "\n",
    "# text = \"\"\n",
    "text = combined_string\n",
    "tokenizer = Tokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Prepare data for CSV\n",
    "rows = [[token.surface, token.part_of_speech] for token in tokens]\n",
    "\n",
    "# Write to CSV\n",
    "with open('kashi_janome.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Token\", \"Part of Speech\"])  # Writing headers\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open kashi_janome.csv and extract words that is nouns\n",
    "import csv\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "nouns = []\n",
    "with open(\"kashi_janome.csv\", \"r\", newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip headers\n",
    "    for row in reader:\n",
    "        if '名詞' in row[1]:\n",
    "            nouns.append(row[0])\n",
    "\n",
    "print(nouns)\n",
    "\n",
    "# save nouns to csv\n",
    "with open('kashi_nouns.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([nouns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    nouns = []\n",
    "\n",
    "    # Read nouns from kashi_nouns.csv\n",
    "    with open('kashi_nouns.csv', 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Caution: random sampling is used here!!!!!!!!!\n",
    "            if random.random() < 0.3:\n",
    "                nouns.extend(row)\n",
    "\n",
    "    # If nouns is empty, exit\n",
    "    if not nouns:\n",
    "        print(\"No nouns found because of randomness.\")\n",
    "        return\n",
    "\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=900,\n",
    "        height=900,\n",
    "        background_color='white',\n",
    "        font_path='Noto_Sans_JP/NotoSansJP-VariableFont_wght.ttf',\n",
    "        stopwords={\"ん\", \"お\", \"の\", \"こ\", \"（\", \"）\", \"お\", \"さ\"}\n",
    "    ).generate(' '.join(nouns))\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    folder_name = \"wordcloud\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "\n",
    "    # Save word cloud image\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    wordcloud.to_file(f\"./wordcloud/{current_time}.png\")\n",
    "\n",
    "    # Display word cloud\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "\n",
    "NUM = 5\n",
    "for _ in range(NUM):\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
